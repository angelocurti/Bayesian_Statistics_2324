# -*- coding: utf-8 -*-
"""CAVI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/angelocurti/Bayesian_Statistics_2324/blob/main/notebooks/CAVI.ipynb
"""

import jax.numpy as jnp
from jax import grad, jit, vmap
from jax import random
import numpy as np
import jax
import scipy
from sklearn.metrics import adjusted_rand_score
!pip install mycolorpy
from scipy.stats import multivariate_normal

"""# Generazione Dati"""

from IPython.utils.sysinfo import num_cpus

# numero di cluster
K=5

# dimensione del campione
d=2

# numero di sample
N=100

seed=2022
key = random.PRNGKey(seed)
# vectors of mean of clusters
sigma=5
mu=random.normal(key,(K,d))*sigma

# cluster assignment
key = random.PRNGKey(2)
c=random.categorical(key,(1/K)*jnp.ones(shape=(K,)),axis=0,shape=(N,))
C=np.zeros(shape=(N,K))
for i in range(N):
  C[i,c[i]]=1
C=jnp.array(C)

# Data
X=jnp.matmul(C,mu)+random.normal(key,(N,d))

import matplotlib.pyplot as plt

for i in range(K):
  plt.scatter(X[c==i,0],X[c==i,1])
plt.show()

"""# Variational Inference

We construct the model for our VI algorithm, we stick to the notation of the paper. Our model is described by:

\begin{align*}
    \mu_k\ \mid x &\stackrel{\tiny\mbox{ind}}{\sim} \mathcal{N}_d\left(m_k, s^2_k\ \mathbb{I}_d\right) \qquad \qquad \ \ \  k=1,...,K\\
    x_i \mid c_i,μ &\stackrel{\tiny\mbox{ind}}{\sim} \mathcal{N}_d\left(c^T_iμ, \ \mathbb{I}_d\ \right) \qquad  \qquad \ \ \ \  i=1,...,n \\
    \mu_k &\stackrel{\tiny\mbox{iid}}{\sim}\mathcal{N}_d\left(0, \ \sigma^2\  \mathbb{I}_d\right) \qquad \qquad \ \ \ \   k=1,...,K\\
    c_ i&\stackrel{\tiny\mbox{iid}}{\sim} Cat(\frac{1}{K},...,\frac{1}{K}) \qquad \qquad  i=1,...,n
\end{align*}

Considering $q(c_i=k)$=$𝝋_{ik}$ we can define the variational update for the ith cluster assignment as

\begin{align*}
  𝜑_{ik}∝exp[\ \mathbb{E}[\mu_k;\ m_k,\ s_k^2]x_i-\mathbb{E}[\mu_k^2;\ m_k,\ s_k^2]/2\ ]
\end{align*}

Then we can define the updates for the variational density of the kth mixture component q($\mu_k$) in terms of variational mean and variance


\begin{align*}
    \hat{m_k}=\frac{\sum_{i}^{}𝝋_{ik}x_i}{\frac{1}{\sigma^2}+\sum_{i}^{}𝝋_{ik}} \qquad \qquad \hat{s_k}^2=\frac{1}{\frac{1}{\sigma^2}+\sum_{i}^{}𝝋_{ik}}
\end{align*}


VI can be considered as an optimization problem, so we need to define the function to be optimized, which is the ELBO.
\begin{align*}
    ELBO(\hat{m},\hat{s}^2,𝜑)=\sum_{k=1}^{K}\mathbb{E}[\log p(\mu_k);\ \hat{m_k},\ \hat{s_k}^2] \  +\\  \sum_{i=1}^{n}(\ \mathbb{E}[\log p(c_i);\ 𝜑_i]+\mathbb{E}[\log p(x_i|c_i,μ);\ 𝜑_i,\ \hat{m},\ \hat{s}^2]\ )\ + \  \\ - \sum_{i=1}^{n} \mathbb{E}[\log q(c_i;\ 𝜑_i)]- \sum_{k=1}^{K} \mathbb{E}[\log q(\mu_k;\ \hat{m_k},\ \hat{s_k}^2)]
\end{align*}

\\
Considering $k=1,...,K$ and $i=1,...,n$
"""

#def update_phi(data,phi,m,s2):
#  for i in jnp.arange(data.shape[0]):
#    for k in jnp.arange(phi.shape[1]):
#      phi[i,k]=jnp.exp(jnp.matmul(m[k,:],data[i,:].transpose())-(s2[k]+jnp.matmul(m[k,:],m[k,:].transpose()))/2) # non sono così sicuro della formula per mk^2
#    phi[i,:]=phi[i,:]/jnp.sum(phi[i,:])
#  return phi

def update_phi(data, phi, m, s2):
    M=jnp.matmul(m,m.T)
    log_likelihood = jnp.matmul(data,m.T) - 0.5 * jnp.matmul(jnp.ones(shape=(N,1)),(s2.T + jnp.resize(jnp.diag(M),(K,1)).T))
    # updated_phi=scipy.special.softmax(log_likelihood)
    updated_phi = jnp.nan_to_num(jnp.exp(log_likelihood),nan=10**-10,posinf=10**5) + 10**-10 # to avoid round zeros that make the ELBO go nan

    updated_phi /= jnp.sum(updated_phi,axis=1,keepdims=True)


    def true_fun(updated_phi):
      updated_phi +=10**-10
      updated_phi /= jnp.sum(updated_phi,axis=1,keepdims=True)
      return updated_phi

    def false_fun(updated_phi):
      return updated_phi

    updated_phi=jax.lax.cond((jnp.sum(updated_phi==0)>0), true_fun, false_fun,updated_phi)

    return updated_phi

update_phi_jit=jit(update_phi)


def update_mean_and_variance(data,phi,sigma):
  K=phi.shape[1]
  d=data.shape[1]
  N=data.shape[0]
  updated_m=(jnp.matmul(phi.T,data)/(1/sigma**2*jnp.ones(shape=(K,d))+jnp.matmul(jnp.resize(jnp.sum(phi,axis=0),(1,K)).T,jnp.ones(shape=(1,d)))))
  updated_s2=(1/(1/sigma**2*jnp.ones(shape=(K,1))+jnp.resize(jnp.sum(phi,axis=0),(1,K)).T))
  return updated_m,updated_s2

update_mean_and_variance_jit=jit(update_mean_and_variance)


def compute_ELBO(m,s2,phi,data):
  # when computing the ELBO value, we omit constants because once we compute the improvement they would have a total of 0
  # Fn stands for the nth component of the formula (21) in the review paper
  d=m.shape[1]
  M=jnp.matmul(m,m.T)

  F1=-0.5/sigma**2 *jnp.sum( d*s2+jnp.diag(M))

  # F2= -log(K) sum over k from 1 to K => constant in every iteration

  F3=-0.5*jnp.sum(jnp.matmul(phi.T,jnp.diag(jnp.matmul(data,data.T))))+jnp.sum(jnp.diag(jnp.matmul(phi.T,jnp.matmul(data,m.T))))-0.5*d*jnp.sum(jnp.matmul(phi,s2))
  F3+=-0.5*jnp.sum(jnp.matmul(phi,jnp.diag(M)))
  # -d/2*jnp.log(2*jnp.pi)*phi[i,k] summed over i and k should be constant over time, since phi[i,:] is a probability it should sum N every time

  F4=jnp.sum(jnp.log(phi)*phi)

  F5=-d/2*jnp.sum(jnp.log(s2))

  return F1+F3-F4-F5

compute_ELBO_jit=jit(compute_ELBO)

# FUNCTION FOR VARIATIONAL INFERENCE
# Notation of the paper

def single_iteration_VI(data,K,sigma,i,nMAX,tol):
    N=data.shape[0]
    d=data.shape[1]
    key = random.PRNGKey(i*seed)
    phi=random.uniform(key,minval=0,maxval=1,shape=(N,K))
    phi/=jnp.sum(phi,axis=1,keepdims=True)
    m=random.normal(key,shape=(K,d))*sigma
    s2=random.uniform(key,minval=0,maxval=10,shape=(K,1))
    improvement=1
    ELBO_old=0 # probabilmente questo andrà modificato
    ELBO_new=compute_ELBO_jit(m,s2,phi,data)
    nit=0

    def cond(state):
      _,_,_,_,_,nit,ELBO,improvement,tol,nMAX=state
      return (((improvement>tol) & (nit<nMAX)) | (ELBO==jnp.nan))

    def iteration_while(state):
      data,m,s2,phi,sigma,nit,ELBO_new,improvement,tol,nMAX=state
      phi=update_phi_jit(data,phi,m,s2)
      m,s2=update_mean_and_variance_jit(data,phi,sigma)
      ELBO_old=ELBO_new
      ELBO_new=compute_ELBO_jit(m,s2,phi,data)
      improvement=ELBO_new-ELBO_old
      nit+=1
      return (data,m,s2,phi,sigma,nit,ELBO_new,improvement,tol,nMAX)

    state=(data,m,s2,phi,sigma,nit,ELBO_new,improvement,tol,nMAX)
    _,m,s2,phi,_,nit,ELBO_new,_,_,_=jax.lax.while_loop(cond_fun=cond,body_fun=iteration_while,init_val=state)

      #print('Iter ',nit,'\t ELBO: ',ELBO_new,'\t Improvement: ',improvement,'\n')
      #print('=================================================\n')
    return  m,s2,phi,ELBO_new,nit

single_iteration_VI_jit=jit(single_iteration_VI,static_argnames=['K','nMAX','tol'])


def VI(data,K,sigma,nMAX,n_iniz,tol):
  # creating our variables as estimation of parameters for posterior probabilities
  # jax arrays are immutable, so I don't know how to create these variables in jax and use them
  # I iniialize them randomly, since there is no a-priori starting point which is best than others
  ELBO_max=0
  for i in range(n_iniz):
    m,s2,phi,ELBO_new,nit=single_iteration_VI_jit(data,K,sigma,i,nMAX,tol)
    if i==0:
      ELBO_max=ELBO_new
      m_max=m
      s2_max=s2
      phi_max=phi
      n_max=0
    print('Initialization number: ',i+1,'\t ELBO: ',ELBO_new,'\t N_iterations: ',nit)
    print('=================================================\n')
    if ELBO_new>ELBO_max:
      ELBO_max=ELBO_new
      m_max=m
      s2_max=s2
      phi_max=phi
      n_max=i
  print('Best initialization at ',n_max+1,' \t ELBO: ',ELBO_max,'\n\n')
  return m_max,s2_max,phi_max

VI_jit=jit(VI)

import matplotlib.pyplot as plt
from mycolorpy import colorlist as mcp

def plot_clusters(data, phi, means):
    # Get the cluster assignments
    cluster_assignments = jnp.argmax(phi, axis=1)

    # Number of clusters
    num_clusters = means.shape[0]

    # Create a list of colors for plotting
    #colors = plt.colormaps()
    colors=mcp.gen_color(cmap="viridis",n=K)
    print(colors)

    # Scatter plot each data point with its assigned color
    plt.figure(figsize=(8, 6))
    for cluster in range(num_clusters):
        plt.scatter(data[cluster_assignments == cluster, 0], data[cluster_assignments == cluster, 1], c=colors[cluster], label=f'Cluster {cluster}')

    # Plot the cluster means
    for cluster in range(num_clusters):
        plt.scatter(means[cluster, 0], means[cluster, 1], marker='x', color='red', s=100, label=f'Mean Cluster {cluster}')

    plt.legend()
    plt.title('Cluster Visualization')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.show()

# %%timeit
tol=10**-16
m,s2,phi=VI(X,K,sigma,10000,100,tol)
#print(m,'\n',mu)
#print(s2)
#print(phi,'\n',C)

# Eslicitiamo la likehood di muk rispetto ai dati
n_cluster = np.sum(C, axis = 0)
m_post=1/(1+1/sigma**2)/np.matmul(n_cluster.reshape((K,1)),np.ones(shape=(1,d)))*np.matmul(C.T,X)
s2_post=1/(1+1/sigma**2)/n_cluster

plot_clusters(X, phi, m)

for i in range(K):
  plt.scatter(X[c==i,0],X[c==i,1])
  plt.scatter(m_post[i, 0], m_post[i, 1], marker='x', color='red', s=100, label=f'Mean Cluster {i}')

plt.show()

"""
Il codice che segue è un'idea di metrica di errore"""

def error_measure(m_real,s2_real,clus_real,m_pred,s2_pred,clus_pred):
  K=clus_pred.shape[1]
  N=clus_pred.shape[0]
  weights1=1/K*np.ones((K,))
  weights2=np.mean(clus_pred,axis=1)
  mix1=0
  mix2=0
  inter=0
  d=m_real.shape[1]

  for i in np.arange(K):
    for j in np.arange(K):
      s2_ij=s2_real[i]+s2_real[j]
      mix1+=weights1[i]*weights1[j]*multivariate_normal.pdf(m_real[i,],m_real[j,],s2_ij*np.eye(d))
      s2_ij=s2_pred[i]+s2_pred[j]
      mix2+=weights2[i]*weights2[j]*multivariate_normal.pdf(m_pred[i,],m_pred[j,],s2_ij*np.eye(d))
      s2_ij=s2_real[i]+s2_pred[j]
      inter+=weights1[i]*weights2[j]*multivariate_normal.pdf(m_real[i,],m_pred[j,],s2_ij*np.eye(d))

  error_m_s2=mix1+mix2-2*inter
  clus_real_num=np.empty((N,))
  clus_pred_num=np.empty((N,))
  for i in np.arange(N):
    clus_real_num[i]=np.argmax(clus_real[i,])
    clus_pred_num[i]=np.argmax(clus_pred[i,])

  error_clust=1-adjusted_rand_score(clus_real_num,clus_pred_num)
  return error_m_s2,error_clust

error_m_s2,error_clust=error_measure(m_post,s2_post,C,m,s2,phi)
error_m_s2,error_clust

"""## VALUTAZIONE ERRORE E TEMPO"""

# numero di cluster
K=5

# dimensione del campione
d=15

# Numero di sample
N=1000

seed=2022
key = random.PRNGKey(seed)
# vectors of mean of clusters
sigma=5
mu=random.normal(key,(K,d))*sigma

# cluster assignment
key = random.PRNGKey(2)
c=random.categorical(key,(1/K)*jnp.ones(shape=(K,)),axis=0,shape=(N,))
C=np.zeros(shape=(N,K))
for i in range(N):
  C[i,c[i]]=1
C=jnp.array(C)

# Data
X=jnp.matmul(C,mu)+random.normal(key,(N,d))

# Commented out IPython magic to ensure Python compatibility.
# %%timeit
# tol=10**-16
# m,s2,phi=VI(X,K,sigma,10000,100,tol)

n_cluster = np.sum(C, axis = 0)
m_post=1/(1+1/sigma**2)/np.matmul(n_cluster.reshape((K,1)),np.ones(shape=(1,d)))*np.matmul(C.T,X)
s2_post=1/(1+1/sigma**2)/n_cluster
error_m_s2,error_clust=error_measure(m_post,s2_post,C,m,s2,phi)
error_m_s2,error_clust