{"cells":[{"cell_type":"code","execution_count":54,"id":"4ee7630e-9898-4e0c-92ca-4180a0cf21a0","metadata":{"id":"4ee7630e-9898-4e0c-92ca-4180a0cf21a0","executionInfo":{"status":"ok","timestamp":1699114039434,"user_tz":-60,"elapsed":9,"user":{"displayName":"Alessandro Ciceri","userId":"09562526887450501692"}}},"outputs":[],"source":["import jax.numpy as jnp\n","from jax import grad, jit, vmap\n","from jax import random\n","import numpy as np"]},{"cell_type":"markdown","source":["# Generazione Dati"],"metadata":{"id":"j896nV5HKHJT"},"id":"j896nV5HKHJT"},{"cell_type":"code","execution_count":57,"id":"c0a3975e-55bb-4730-988a-abf1675cea01","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c0a3975e-55bb-4730-988a-abf1675cea01","executionInfo":{"status":"ok","timestamp":1699114114404,"user_tz":-60,"elapsed":773,"user":{"displayName":"Alessandro Ciceri","userId":"09562526887450501692"}},"outputId":"add2c4e7-d843-4aac-ec53-144a2a1a5082"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Array([[ 0.90678126,  0.44637144],\n","       [-0.6740475 ,  0.31190777],\n","       [-1.8890792 ,  1.2733475 ],\n","       ...,\n","       [-0.38996434, -0.1008999 ],\n","       [ 0.58381635, -0.42401785],\n","       [-0.79596436, -0.98130786]], dtype=float32)"]},"metadata":{},"execution_count":57}],"source":["from IPython.utils.sysinfo import num_cpus\n","\n","# numero di cluster\n","K=5\n","\n","# dimensione del campione\n","d=2\n","\n","# numero di sample\n","N=1000\n","\n","key = random.PRNGKey(2023)\n","# vectors of mean of clusters\n","mu=random.normal(key,(K,d))\n","\n","# cluster assignment\n","key = random.PRNGKey(2)\n","c=random.categorical(key,(1/K)*jnp.ones(shape=(K,)),axis=0,shape=(N,))\n","C=np.zeros(shape=(N,K))\n","for i in range(N):\n","  C[i,c[i]]=1\n","C=jnp.array(C)\n","\n","# Data\n","X=jnp.matmul(C,mu)+random.normal(key,(N,d))"]},{"cell_type":"markdown","source":["# Variational Inference\n","\n","We construct the model for our VI algorithm, we stick to the notation of the paper. Our model is described by:\n","\n","\\begin{align*}\n","    \\mu_k\\ \\mid x &\\stackrel{\\tiny\\mbox{iid}}{\\sim} \\mathcal{N}\\left(m_k, s^2_k\\right) \\\\\n","    x_i \\mid c_i,μ &\\sim \\mathcal{N}\\left(c^T_iμ, 1\\right) \\\\\n","    \\mu_k &\\stackrel{\\tiny\\mbox{iid}}{\\sim} \\mathcal{N}\\left(0, \\sigma^2\\right) \\\\\n","\\end{align*}\n","\n","We work in the family of Gaussian distribution for this first attempt, then we could expand it to exponential family.\n"],"metadata":{"id":"MWVlPUiOHptl"},"id":"MWVlPUiOHptl"},{"cell_type":"markdown","source":["Il paper descrive l'algoritmo in un caso 1D, dobbiamo adattarlo in un generico caso multidimensionale, io ho iniziato implementando il caso unidimensionale nell'algoritmo copiandolo dal paper, consapevole che andrà adattato"],"metadata":{"id":"69TrgD2pW1Ku"},"id":"69TrgD2pW1Ku"},{"cell_type":"code","source":["def update_phi(data,phi,m,s2):\n","  for i in jnp.arange(data.shape[0]):\n","    for k in jnp.arange(phi.shape[1]):\n","      phi[i,k]=jnp.exp(jnp.matmul(m[k,],data[i,].transpose())-(s2[k]+jnp.matmul(m[k,],m[k,].transpose()))/2) # non sono così sicuro della formula per mk^2\n","    phi[i,]=phi[i,]/jnp.sum(phi[i,])\n","  return phi\n","\n","update_phi_jit=jit(update_phi)\n","\n","def update_mean_and_variance(data,phi,m,s2,sigma):\n","  for k in jnp.arange(phi.shape[1]):\n","    m[k]=jnp.matlmul(phi[:,k].transpose(),data)/(1/sigma**2+jnp.sum(phi[:,k]))\n","    s2[k]=1/(1/sigma**2+jnp.sum(phi[:,k]))\n","  return m,s2\n","\n","update_mean_and_variance_jit=jit(update_mean_and_variance)\n","\n","def compute_ELBO(m,s2,phi):\n","\n","  return\n","\n","compute_ELBO_jit=jit(compute_ELBO)\n","\n"],"metadata":{"id":"Bc6iHqDpT7_r","executionInfo":{"status":"ok","timestamp":1699119296799,"user_tz":-60,"elapsed":3,"user":{"displayName":"Alessandro Ciceri","userId":"09562526887450501692"}}},"id":"Bc6iHqDpT7_r","execution_count":64,"outputs":[]},{"cell_type":"code","source":["# FUNCTION FOR VARIATIONAL INFERENCE\n","# Notation of the paper\n","def VI(data,K,sigma):\n","  # creating our variables as estimation of parameters for posterior probabilities\n","  # jax arrays are immutable, so I don't know how to create these variables in jax and use them\n","  # I iniialize them randomly, since there is no a-priori starting point which is best than others\n","  N=data.shape[0]\n","  d=data.shape[1]\n","  phi=np.zeros((N,K))\n","  for i in range(N):\n","    phi[np.random.categorical(K,p=1/K*np.ones(K))]=1\n","  m=np.random.normal(size=(K,d))\n","  s2=np.random.uniform(0,10,size=(K,))\n","  improvement=1\n","  tol= 10**-10\n","  ELBO_old=0 # probabilmente questo andrà modificato\n","  ELBO_new=0\n","  nit=0 # number of iterations\n","  while improvement>tol:\n","    phi=update_phi_jit(data,phi,m,s2)\n","    m,s2=update_mean_and_variance_jit(data,phi,m,s2,sigma)\n","    ELBO_old=ELBO_new\n","    ELBO_new=compute_ELBO_jit(m,s2,phi)\n","    improvement=jnp.abs(ELBO_new-ELBO_old)\n","    nit+=1\n","  return m,s2,phi\n","\n","\n"],"metadata":{"id":"gByjI96oGIPv","executionInfo":{"status":"ok","timestamp":1699117064529,"user_tz":-60,"elapsed":381,"user":{"displayName":"Alessandro Ciceri","userId":"09562526887450501692"}}},"id":"gByjI96oGIPv","execution_count":60,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}